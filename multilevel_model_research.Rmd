---
title: "Multilevel Models - Applications to Human Factors Research"
author: "Courtney Goodridge"
date: "17/11/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This script contains the analysis for the manuscripts: *Multilevel Models: Applications and Uses in Human Factors Research*

Data for the first analysis is from the Mole et al (2020) manuscript. This manuscript reference is: Mole, C., Pekkanen, J., Sheppard, W., Louw, T., Romano, R., Merat, N., ... & Wilkie, R. (2020). Predicting takeover response to silent automated vehicle failures. Plos one, 15(11), e0242825. 

Data and analysis scripts can be found on OSF: https://osf.io/aw8kp/. 


First we load the necessary packages needed for this analysis

## Packages

```{r}
if(!require(here)) install.packages("here")
library(here)

if(!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)

if(!require(dplyr)) install.packages("dplyr")
library(dplyr)

if(!require(tidyr)) install.packages("tidyr")
library(tidyr)

if(!require(viridis)) install.packages("viridis")
library(viridis)

if(!require(lme4)) install.packages("lme4")
library(lme4)

if(!require(data.table)) install.packages("data.table")
library(data.table)

if(!require(emmeans)) install.packages("emmeans")
library(emmeans)

if(!require(patchwork)) install.packages("patchwork")
library(patchwork)

if(!require(marginaleffects)) install.packages("marginaleffects")
library(marginaleffects)

if(!require(purrr)) install.packages("purrr")
library(purrr)
```

## Data

```{r}
steering_data <- fread(file = here::here("ITS/Multilevel models paper/Mole et al (2020) data and analysis/collated_steering.csv"))
```

## Plot theme

Theme for all the plots in this manuscript

```{r}
theme_plot <- theme(axis.title.x = element_text(size = 15), axis.text.x = element_text(size = 15), axis.title.y = element_text(size = 15), axis.text.y = element_text(size = 15), title = element_text(size = 18), legend.title = element_text(size = 10), legend.text = element_text(size = 10), panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"))
```

## Calculating reaction time

Now the data has been loaded, the reaction time can be calculated. I calculate steering wheel angle and mirror the directional variables for completeness but this is not strictly necessary (only TLC_takeover will be used for the analysis). 

```{r}
# Mole et al (2020) calculating the SWA. I won't be using that for the analysis, but for completeness I calculate it here.
steering_data <- steering_data %>%
  dplyr::rename(swv = swa) %>%
  dplyr::mutate(swa = swv * 90)

# Next the data mirrored. Again, I am only focusing on timing variable so these won't be needed for the analysis but are included for completeness.
steering_data <- steering_data %>% 
  dplyr::mutate(world_x_mirrored = if_else(bend == -1, world_x * -1, world_x),
                swa_mirrored = if_else(bend == -1, swa * -1, swa),
                swv_mirrored = if_else(bend == -1, swv * -1, swv),
                sb_mirrored = if_else(bend == -1, steeringbias * -1, steeringbias))

# function for calculating the TLC from the offset where is varies 
TLC_from_offset <- function(b, w = 1.5, r = 80, v = 8){
  b = b / 180 * pi
  
  TLC = sqrt(w*(2*r + sign(b)*w)/(abs(b)*r*v))
}

steering_data <- steering_data %>% 
  dplyr::mutate(TLC_calc = TLC_from_offset(sab))


#add RT and disengage flag. This code calculates RT. I had to alter it slightly to include an if statement where if the auto_false was length 0 (i.e., a person never disengaged), it was set to NA instead of being empty. This allows the is.na() function to work in the following lines of code)
disengage_RT <- function(onsettime, timestamp_trial, autoflag){
  
  #pick first frame where autoflag == false, then take the timestamp and minus the onset_time
  auto_false <- which(autoflag == FALSE) # 1) select the autoflags which are false
  if(length(auto_false) == 0){ # 2) if there is no false autoflag, set auto_false to NA
    auto_false = NA
  }
  disengage_index <- first(auto_false) # 3 select the first frame that automation was disengaged
  disengage_trialtime <- timestamp_trial[disengage_index] # 4) select the trial time of the automation disengagement
  onset_time <- first(onsettime) # 5 Select the first onset time
  RT <- disengage_trialtime - onset_time # 6 subtract onset time from disengage trial time (can be negative)
  return(RT) # return the RT
  
}

# calculate RT
steering_data <- steering_data  %>% 
  dplyr::group_by(ppid, sab, cogload, trialn) %>% 
  dplyr::mutate(RT = disengage_RT(onsettime, timestamp_trial, autoflag),
         disengaged = ifelse(is.na(RT), 0, 1) # whether or not they actually took over.
  )

# change cogload variable into a factor
steering_data$cogload <- as.factor(steering_data$cogload)


#rename cogload factors so that it makes sense - no load and load
steering_data$cogload <- plyr::mapvalues(steering_data$cogload, from = c("None", "Middle"), to = c("noload", "load"))

#create unique trial id
steering_data <- steering_data %>% 
  dplyr::mutate(trialid = paste(ppid, cogload, trialn, sep = "_"))

# select reaction times for each condition
data_RTs <- steering_data %>% 
  dplyr::ungroup() %>% 
  dplyr::group_by(ppid, cogload, trialn) %>% 
  dplyr::summarise(RT = first(RT),
                   disengaged = first(disengaged), #whether or not they actually took over.
                   premature = ifelse(RT <= 0, 1, 0),
                   sab = first(sab),
                   maxsb = max(abs(sb_mirrored)),
                   onsettime = first(onsettime),
                   design = first(design),
                   #simTLC = first(simulated_TLC),
                   autofile_i = first(autofile_i),
                   TLC_calc = first(TLC_calc))
```

## Calculating TLC_takeover

To calculate the TLC_takeover, the simulated timings for the silent failures need to be merged with the reaction times. This is done in the following chunk. 

```{r}
# loading simualted_TLC
sim_TLCs <- fread(file = here::here("ITS/Multilevel models paper/Mole et al (2020) data and analysis/simulated_ttlcs.csv"))

# merge simulated TLCs with RT dataframe
data_RTs <- merge(data_RTs, sim_TLCs, by = c("sab","onsettime","autofile_i"))

# calculating TLC_takeover
data_RTs <- data_RTs %>% 
  dplyr::mutate(TLC_onset = simulated_ttlc,
                TLC_takeover = TLC_onset - RT,
                time_remaining = 15 - onsettime,
                early = time_remaining - RT,
                TLC_end_of_trial = TLC_onset - time_remaining)  

# filter out conditions where takeovers did not happen, or they happened too early (i.e., before the onset of the failure)
data_RTs_cens <- data_RTs %>%
  dplyr::filter(premature == 0 | is.na(premature)) %>% 
  dplyr::mutate(censored = ifelse(is.na(TLC_takeover), -1,
                                  ifelse(TLC_takeover < TLC_end_of_trial, -1, 0)))

data_RTs_cens$cogload <- relevel(data_RTs_cens$cogload, "noload")

#13 moves the wheel pretty much every trial before they should do so they are filtered out
data_RTs_cens <- dplyr::filter(data_RTs_cens, ppid != 13) 

# removing NAs (i.e, when people did not disengage) and only concentrating on no load conditions
data_RTs_cens_NA_removed <- data_RTs_cens %>%
  dplyr::filter(disengaged == 1, cogload == "noload")

# only focusing on balanced conditions and rounding the onset values
data_RTs_cens_NA_removed <- data_RTs_cens_NA_removed %>% 
  dplyr::filter(design == "balanced") %>% 
  dplyr::mutate(TLC_failure = round(TLC_onset, digits = 2)) 
```

## Distribution of TLC_takeover

The distribution of TLC_takeover reasonably follows a Gaussian shape, however there is some skew (positively skewed with an extended right tail). This suggests that a Gaussian distribution might not be the best to model TLC_takeover. However it should be noted that it is the residuals that should be normally distributed, rather than the response metric itself. We'll go forward with a Gaussian distribution and inspect the fit. 

```{r}
# Overall distribution of TLC_takeover
ggplot(data = data_RTs_cens_NA_removed, mapping = aes(x = TLC_takeover)) +
  geom_histogram(alpha = 0.5, bins = 50, col = "black") +
  ggtitle(expression("Overall distribution of" ~ TLC[t]))

# Distribution of TLC_takeover within each failure severity condition
ggplot(data = data_RTs_cens_NA_removed, mapping = aes(x = TLC_takeover)) +
  geom_histogram(aes(fill = as.factor(TLC_failure)), alpha=0.5, bins=50, col = "black") +
  facet_wrap(~ TLC_failure) +
  ggtitle("Distribution within each failure condition")
```

## Fitting an ordinary linear regression

```{r}
# fitting a linear regression model
mod_1<- lm(TLC_takeover ~ TLC_failure, data = data_RTs_cens_NA_removed)

# summarizing the linear regression model to see the parameter estimates
summary(mod_1)

# confidence intervals for the linear regression
confint(mod_1)
```

## Investigating the residual errors

```{r}



```

## Does log transforming the response variable improve the fit?

```{r}




```

## Regression model fit

For plotting the model parameters, I will also want to compute confidence intervals around my estimate of the mean. To do this for the linear models and the multilevel models, I follow a tutorial by Solomon Kurz (https://solomonkurz.netlify.app/blog/2021-12-16-use-emmeans-to-include-95-cis-around-your-lme4-based-fitted-lines/). This is a really good tutorial and involves using the *emmean()* function from the emmeans package.

Within the *emmeans()* function, I specify failure criticality _(ttlc.failure)_ for the specs and in my list, I calculate predictions for failure criticalities between 1 s and 3 s. This is outside the range of failures used within the experiment, but because the data appear linear, we can extend out predictions to predict what ttlc.takeover values would be predicted for failures of 3 s.  


```{r}
# plotting linear regression model over data
linear_regression_plot <- ggplot() +
  geom_point(data_RTs_cens_NA_removed, mapping = aes(x = TLC_failure, TLC_takeover), position = position_jitter(seed = 42, width = 0.4), alpha = 0.2, size = 1.5) +
  geom_ribbon(data = linear_regression_emmeans, mapping = aes(x = TLC_failure, y = emmean, ymin = lower.CL, ymax = upper.CL), alpha = .5) +
  geom_line(linear_regression_emmeans, mapping = aes(x = TLC_failure, y = emmean), linetype = "dashed", col = "red") +
  geom_point(data_RTs_cens_NA_removed %>%
               dplyr::group_by(TLC_failure) %>%
               dplyr::summarise(mean_takeover = mean(TLC_takeover)), mapping = aes(x = TLC_failure, y = mean_takeover), fill = "white", pch = 21, size = 1.5) + 
  ggtitle("A") +
  xlab(expression(TLC[F] ~ "(s)")) +
  ylab(expression(TLC[T] ~ "(s)")) +
  ylim(0, 7) +
  xlim(0, 11) +
  theme_plot

# number of observations per participant
data_RTs_cens_NA_removed %>%
  dplyr::group_by(ppid) %>%
  dplyr::summarise(obs = n()) %>%
  View()
```

## Non-independence 

The data points are not independent individual each because each participants has multiple observations within a condition. Data points belonging to one individual a more closely related to their other data points in comparison to someone else's data points. To highlight this, we select the first 5 participants in different colours. Participant 

```{r}
# example of 5 participants
subset_sample_plot <- ggplot() +
  geom_point(data_RTs_cens_NA_removed %>%
         dplyr::filter(ppid <= 5), mapping = aes(x = TLC_failure, TLC_takeover, col = as.factor(ppid)), position = position_jitter(seed = 42, width = 0.3), alpha = .8, size = 1.5) +
  xlab(expression(TLC[F] ~ "(s)")) +
  ylab(expression(TLC[T] ~ "(s)")) +
  scale_color_discrete(name = "Participant") +
  ggtitle("B") +
  ylim(0, 7) +
  xlim(0, 11) +
  theme_plot +
  theme(legend.position = c(.2, .6), legend.key.width = unit(0.1, 'cm'), legend.key.height = unit(0.1, 'cm'), legend.key = element_blank())

# how many observations per participant
data_RTs_cens_NA_removed %>%
  dplyr::filter(ppid <= 5) %>%
  dplyr::group_by(ppid) %>%
  dplyr::summarise(obs = n()) %>%
  View()

# how many participants
data_RTs_cens_NA_removed %>%
  dplyr::filter(ppid <= 5) %>%
  View()
```

## Figure 1

Left panel contains all data the parameters of the model predicting sample means in each condition. Right panel contains an example of 5 participants to illustrate the violation of independence amongst the data points

```{r}
fig_1 <- linear_regression_plot | subset_sample_plot

ggsave(here::here("multilevel_model_HF_applications/plots/fig_1.tiff"), plot = fig_1, width = 16, height = 8, units = 'cm', dpi = 300, type = 'cairo')
```

## Fitting a varying intercept model

Here we fit a varying intercept model. The *(1 | ppid)* part of the model equations fits an intercept for each participant. This accounts for the non-independence of observations by constraining the non-independent clusters to the same intercept. Remember, this is something we need to do as each member of the random effects group has multiple observations. 

```{r}
# fitting a varying intercept model
mod_2 <- lme4::lmer(TLC_takeover ~ TLC_failure + (1 | ppid), data = data_RTs_cens_NA_removed)

# summarizing the model
summary(mod_2)

# generating confidence intervals
confint(mod_2)

coef(mod_2)[1]
```

## Figure 2

In a previous plot, it was highlighted how the repeated observations for each participant within each condition violated the non-independence assumption. Now each participant had been modeled with an intercept, we can update that plot and assign each participant with their own intercept. You'll notice that there are only 4 regression lines. This is because participant 1 and 2 have very similar intercept values, and because have modelled everyone as having a fixed TLC_failure slope, this is exactly the same for everyone. 

This might seem strange given that the scatter plot seems to show that the two participants respond differently as failure get less severe. This is one indication that random slopes might be necessary. 

```{r}
# Computing 95% CIs using emmeans() function for linear regression model
varying_intercept_emmeans <- emmeans(object = mod_2, specs = ~ TLC_failure, at = list(TLC_failure = seq(1.5, to = 10.5, by = .1))) %>% 
  data.frame()

# model predictions - predictions for TLC_t for failure severities between 1.5 s and 10.5 for 5 participants in the model
TLC_failure <- data.frame(TLC_failure = seq(1.5, to = 10.5, by = .1))
ppid <- data.frame(ppid = seq(1, to = 5, by = 1))
pred_list <- expand_grid(ppid, TLC_failure)

# 5 participants we are focusing on
example_5_participants <- data_RTs_cens_NA_removed %>%
  dplyr::filter(ppid <= 5)

mod_2_predict_ppids <- predict(mod_2, newdata = pred_list) %>% 
  data.frame() %>% 
  set_names("y_hat") %>% 
  bind_cols(pred_list)

# example of 5 participants with their specifc intercepts
fig_2 <- ggplot() +
  geom_point(data_RTs_cens_NA_removed %>%
         dplyr::filter(ppid <= 5), mapping = aes(x = TLC_failure, TLC_takeover, col = as.factor(ppid)), position = position_jitter(seed = 42, width = 0.3), alpha = .8, size = 1.5) +
  geom_line(mod_2_predict_ppids %>%
              dplyr::filter(ppid <= 5), mapping = aes(x = TLC_failure, y_hat, col = as.factor(ppid))) +
  xlab(expression(TLC[F] ~ "(s)")) +
  ylab(expression(TLC[T] ~ "(s)")) +
  scale_color_discrete(name = "Participant") +
  ggtitle("A") +
  ylim(0, 7) +
  xlim(0, 11) +
  theme_plot +
  theme(legend.position = c(.2, .6), legend.key.width = unit(0.1, 'cm'), legend.key.height = unit(0.1, 'cm'), legend.key = element_blank())

ggsave(here::here("multilevel_model_HF_applications/plots/fig_2.tiff"), plot = fig_2, width = 8, height = 8, units = 'cm', dpi = 300, type = 'cairo')
```

## Marginal and conditional effects 

When reading about multilevel models, you will undoubtedly come across "marginal" and "conditional" effects. The terminology can be very confusing (Heiss, 2021), and there are multiple resources on the differences:

https://www.andrewheiss.com/blog/2022/11/29/conditional-marginal-marginaleffects/#conditional-effects-or-effect-of-a-variable-in-an-average-cluster

https://www.andrewheiss.com/blog/2021/11/10/ame-bayes-re-guide/#continuous-effect

https://rpsychologist.com/GLMM-part1-lognormal#simulation-code

To summarise what has been highlighted previously, marginal and conditional effect can be defined as follows:

*Conditional effect*: refers to the average driver i.e, the effect that failure severity has on the average driver. 

*Marginal effect*: the average effect failure severity across all drivers. 

These effects might sound similar but they are subtlety different. In context of the silent failures experiment, a conditional effect refers to the average driver i.e., the fixed effects when the random effects are all set to 0. Hence in our model, this predicts the effect of failure severity on takeover performance for a typical driver. Conversely, a marginal effect refers to the drivers on average. If we simulated 500 new drivers, and took their average on the the response scale, this would represent the population average. 

Luckily, for a linear multilevel model, these conditional effects and marginal effects are equivalent (Magnusson, 2018). 

However if there is a non-linear transformation (i.e, the response variable is log transformed or is approximated by a log normal distribution) then the driver specific and the average driver might not match up. 

It can be demonstrated below that the  the two match up for the current model below. The grand intercept models the fasted response for the typical driver. However if we take the average of the random intercpts, we get identical values. 

```{r}
# grand intercept
fixef(tlc_takeover_varying_intercept)[1]

# average of the random participant intercepts
coef(tlc_takeover_varying_intercept)[1] %>%
  as.data.frame() %>%
  dplyr::summarise(m = mean(ppid..Intercept.))
```

## Plotting fixed effects and participant specific regression lines

By assigning each driver with their own intercept, each driver has their own regression line. Below, we generate regression lines for each driver, and then put them in a plot.

```{r}
# generate new data
failures <- data.frame(TLC_failure = seq(1.5, to = 10.5, by = .1))
ppids <- data.frame(ppid = unique(data_RTs_cens_NA_removed$ppid))
new_values <- expand_grid(failures, ppids)

# predict ppid-level estimates from the multilevel model
ppid_varying_intercepts <- predict(tlc_takeover_varying_intercept, newdata = new_values) %>% 
  data.frame() %>% 
  set_names("y_hat") %>% 
  dplyr::bind_cols(new_values)

# individual intercepts 
varying_intercepts_plot <- ggplot() +
  geom_point(data_RTs_cens_NA_removed %>%
         dplyr::filter(ppid <= 5), mapping = aes(x = TLC_failure, TLC_takeover, col = as.factor(ppid)), position = position_jitter(seed = 42, width = 0.3), alpha = 0.7, size = .9) +
  geom_line(ppid_varying_intercepts %>%
                dplyr::filter(ppid <= 5), mapping = aes(x = TLC_failure, y = y_hat, col = as.factor(ppid))) +
  xlab(expression(TLC[F])) +
  ylab(expression(TLC[T])) +
  scale_color_discrete(name = "Participant") +
  ylim(0, 7) +
  xlim(0, 11) +
  theme_plot +
  theme(legend.position = c(.2, .6), legend.key.width = unit(0.1, 'cm'), legend.key.height = unit(0.1, 'cm'), legend.key = element_blank())
```

## Computing 95% confidence intervals

Next we compute confidence intervals for the fixed effect estiamate and plot it on the raw data. 

```{r}
# Computing 95% confidence intervals for population-level estimates
varying_intercept_emmeans <- emmeans(object = tlc_takeover_varying_intercept, specs = ~ TLC_failure, at = list(TLC_failure = seq(1.5, to = 10.5, by = .1))) %>% 
  data.frame()

# fixed effects with 95% confidence intervals
ggplot() +
   geom_point(data_RTs_cens_NA_removed, mapping = aes(x = TLC_failure, TLC_takeover), position = position_jitter(seed = 42, width = 0.4), alpha = 0.4, size = .7) +
  geom_ribbon(varying_intercept_emmeans , mapping = aes(x = TLC_failure, y = emmean, ymin = lower.CL, ymax = upper.CL), alpha = .5) +
  geom_line(linear_regression_emmeans, mapping = aes(x = TLC_failure, y = emmean), linetype = "dashed") +
    geom_point(data_RTs_cens_NA_removed %>%
               dplyr::group_by(TLC_failure) %>%
               dplyr::summarise(mean_takeover = mean(TLC_takeover)), mapping = aes(x = TLC_failure, y = mean_takeover), fill = "white", pch = 21) +
  xlab(expression(TLC[F])) +
  ylab(expression(TLC[T])) +
  scale_color_discrete(name = "Participant") +
  ylim(0, 7) +
  xlim(0, 11) +
  theme_plot +
  theme(legend.position = c(.2, .6), legend.key.width = unit(0.1, 'cm'), legend.key.height = unit(0.1, 'cm'), legend.key = element_blank())



```

## What actually are random effects?

```{r}
ranef(tlc_takeover_varying_intercept) %>%
  as.data.frame() %>%
  dplyr::mutate(participant_specific_intercept = condval + fixef(tlc_takeover_varying_intercept)[1]) %>%
  View()

ggplot(data_RTs_cens_NA_removed %>%
         dplyr::filter(ppid == 11), mapping = aes(x= TLC_failure, y = TLC_takeover)) +
  geom_point() +
  ylim(0, 4)

```